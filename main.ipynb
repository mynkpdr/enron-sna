{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0eb00d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ENRON EMAIL CORPUS â€” ADVANCED SOCIAL NETWORK ANALYSIS\n",
    "# Google Colab Script  Â·  OPTIMISED FOR SPEED & LOW RAM\n",
    "# ============================================================\n",
    "# Key optimisations vs. original:\n",
    "#   â€¢ CSV streamed in chunks â†’ never loads 1.3 GB into RAM\n",
    "#   â€¢ Header-only regex parser (no email.message_from_string)\n",
    "#   â€¢ Vectorised pandas groupby instead of iterrows\n",
    "#   â€¢ Edge accumulation with flat lists â†’ one-shot DataFrame\n",
    "#   â€¢ Betweenness k=200 sample (fast approx, still accurate)\n",
    "#   â€¢ closeness skipped (O(VÂ·E), too slow at this scale)\n",
    "#   â€¢ avg_clustering on LCC only (not whole graph)\n",
    "#   â€¢ Louvain on LCC only\n",
    "#   â€¢ gc.collect() after each heavy step\n",
    "# ============================================================\n",
    "\n",
    "import re, json, os, gc, warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import community as community_louvain\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "from email.utils import parsedate\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# â”€â”€ Config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "EMAILS_CSV  = \"emails.csv\"\n",
    "OUTPUT_DIR  = \"sna_output\"\n",
    "CHUNK_SIZE  = 20_000       # rows per chunk â€” lower if still OOM\n",
    "ENRON_RE    = re.compile(r'[\\w.+-]+@[\\w.-]*enron\\.com', re.I)\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# FAST HEADER PARSER\n",
    "# Scans only first 2000 chars with regex â€” ~15x faster than\n",
    "# email.message_from_string which also parses the full body.\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "_EMAIL_RE = re.compile(r'[\\w.+-]+@[\\w.-]+\\.[a-z]{2,}', re.I)\n",
    "_FROM_RE  = re.compile(r'^From\\s*:\\s*(.+)',               re.I | re.M)\n",
    "_TO_RE    = re.compile(r'^To\\s*:\\s*(.*?)(?=\\n\\S|\\n\\n|\\Z)',re.I | re.M | re.S)\n",
    "_CC_RE    = re.compile(r'^Cc\\s*:\\s*(.*?)(?=\\n\\S|\\n\\n|\\Z)',      re.I | re.M | re.S)\n",
    "_DATE_RE  = re.compile(r'^Date\\s*:\\s*(.+)',                re.I | re.M)\n",
    "_SUBJ_RE  = re.compile(r'^Subject\\s*:\\s*(.+)',             re.I | re.M)\n",
    "\n",
    "def fast_parse(raw: str):\n",
    "    \"\"\"Return (sender, enron_recips_tuple, date_str, subject) or None.\"\"\"\n",
    "    head = raw[:2000]\n",
    "\n",
    "    m = _FROM_RE.search(head)\n",
    "    if not m:\n",
    "        return None\n",
    "    emails_in_from = _EMAIL_RE.findall(m.group(1))\n",
    "    if not emails_in_from:\n",
    "        return None\n",
    "    sender = emails_in_from[0].lower()\n",
    "    if not ENRON_RE.search(sender):\n",
    "        return None\n",
    "\n",
    "    to_txt  = (_TO_RE.search(head)  or type('',(),{'group':lambda s,i:''})()).group(1) or \"\"\n",
    "    cc_txt  = (_CC_RE.search(head)  or type('',(),{'group':lambda s,i:''})()).group(1) or \"\"\n",
    "    all_recips = _EMAIL_RE.findall(to_txt + \" \" + cc_txt)\n",
    "    enron_recips = tuple(\n",
    "        e.lower() for e in all_recips\n",
    "        if e.lower() != sender and ENRON_RE.search(e)\n",
    "    )\n",
    "    if not enron_recips:\n",
    "        return None\n",
    "\n",
    "    date_m = _DATE_RE.search(head)\n",
    "    date_str = date_m.group(1).strip() if date_m else \"\"\n",
    "    subj_m = _SUBJ_RE.search(head)\n",
    "    subject = subj_m.group(1).strip() if subj_m else \"\"\n",
    "\n",
    "    return sender, enron_recips, date_str, subject\n",
    "\n",
    "def parse_date_ymd(s: str):\n",
    "    try:\n",
    "        t = parsedate(s)\n",
    "        if not t:\n",
    "            return None, None\n",
    "        y = t[0]\n",
    "        if y < 100:\n",
    "            y += 2000 if y < 70 else 1900\n",
    "        return f\"{y:04d}-{t[1]:02d}-{t[2]:02d}\", f\"{y:04d}-{t[1]:02d}\"\n",
    "    except Exception:\n",
    "        return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d4542b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš™ï¸  Streaming CSV â€¦\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "750f0862ad4e441c8180cefc75a15b0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  chunks: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   359,219 valid emails parsed from 517,401 rows\n",
      "   2,347,001 directed edge occurrences collected\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# STREAMING PASS â€” chunk the CSV, parse, accumulate edges\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"âš™ï¸  Streaming CSV â€¦\")\n",
    "\n",
    "src_list = []\n",
    "tgt_list = []\n",
    "w_list = []\n",
    "rc_list = []\n",
    "dc_list = []\n",
    "bc_list = []\n",
    "\n",
    "sent_ctr = Counter()\n",
    "recv_ctr = Counter()\n",
    "month_sets = defaultdict(set)\n",
    "subj_list = []\n",
    "dates_list = []\n",
    "ym_list = []\n",
    "headers_meta = []\n",
    "\n",
    "total_raw = total_ok = 0\n",
    "\n",
    "for chunk in tqdm(\n",
    "    pd.read_csv(\n",
    "        EMAILS_CSV,\n",
    "        usecols=[\"message\"],\n",
    "        chunksize=CHUNK_SIZE,\n",
    "        dtype=str,\n",
    "        engine=\"c\",\n",
    "        on_bad_lines=\"skip\",\n",
    "    ),\n",
    "    desc=\"  chunks\",\n",
    "):\n",
    "    total_raw += len(chunk)\n",
    "    for raw in chunk[\"message\"]:\n",
    "        if not isinstance(raw, str):\n",
    "            continue\n",
    "        result = fast_parse(raw)\n",
    "        if result is None:\n",
    "            continue\n",
    "        sender, enron_recips, date_str, subject = result\n",
    "\n",
    "        date, ym = parse_date_ymd(date_str)\n",
    "        n_r = len(enron_recips)\n",
    "        w = 1.0 / n_r\n",
    "\n",
    "        sent_ctr[sender] += 1\n",
    "        if ym:\n",
    "            month_sets[sender].add(ym)\n",
    "        subj_list.append(subject)\n",
    "        dates_list.append(date)\n",
    "        ym_list.append(ym)\n",
    "\n",
    "        headers_meta.append({\n",
    "            \"date\": date or \"\",\n",
    "            \"from\": sender,\n",
    "            \"to_cc_count\": n_r,\n",
    "            \"subject\": subject,\n",
    "        })\n",
    "\n",
    "        for r in enron_recips:\n",
    "            src_list.append(sender)\n",
    "            tgt_list.append(r)\n",
    "            w_list.append(w)\n",
    "            rc_list.append(1)\n",
    "            dc_list.append(1 if n_r == 1 else 0)\n",
    "            bc_list.append(1 if n_r > 5 else 0)\n",
    "            recv_ctr[r] += 1\n",
    "            if ym:\n",
    "                month_sets[r].add(ym)\n",
    "\n",
    "        total_ok += 1\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"   {total_ok:,} valid emails parsed from {total_raw:,} rows\")\n",
    "print(f\"   {len(src_list):,} directed edge occurrences collected\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a7451a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”— Aggregating edges â€¦\n",
      "   173,899 undirected edges\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# AGGREGATE EDGES â€” vectorised groupby\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"ðŸ”— Aggregating edges â€¦\")\n",
    "\n",
    "df_e = pd.DataFrame(\n",
    "    {\n",
    "        \"src\": pd.array(src_list, dtype=\"string\"),\n",
    "        \"tgt\": pd.array(tgt_list, dtype=\"string\"),\n",
    "        \"w\": np.array(w_list, dtype=\"float32\"),\n",
    "        \"rc\": np.array(rc_list, dtype=\"int32\"),\n",
    "        \"dc\": np.array(dc_list, dtype=\"int32\"),\n",
    "        \"bc\": np.array(bc_list, dtype=\"int32\"),\n",
    "    }\n",
    ")\n",
    "del src_list, tgt_list, w_list, rc_list, dc_list, bc_list\n",
    "gc.collect()\n",
    "\n",
    "# Canonicalise to undirected\n",
    "swap = df_e[\"src\"] > df_e[\"tgt\"]\n",
    "df_e.loc[swap, [\"src\", \"tgt\"]] = df_e.loc[swap, [\"tgt\", \"src\"]].values\n",
    "\n",
    "df_agg = (\n",
    "    df_e.groupby([\"src\", \"tgt\"], sort=False)\n",
    "    .agg(\n",
    "        weight=(\"w\", \"sum\"),\n",
    "        raw_count=(\"rc\", \"sum\"),\n",
    "        direct_count=(\"dc\", \"sum\"),\n",
    "        broadcast_count=(\"bc\", \"sum\"),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "del df_e\n",
    "gc.collect()\n",
    "print(f\"   {len(df_agg):,} undirected edges\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6b1d619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Building graph â€¦\n",
      "   19094 nodes, 173899 edges\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# BUILD GRAPH\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"ðŸ“Š Building graph â€¦\")\n",
    "G = nx.from_pandas_edgelist(\n",
    "    df_agg,\n",
    "    source=\"src\",\n",
    "    target=\"tgt\",\n",
    "    edge_attr=[\"weight\", \"raw_count\", \"direct_count\", \"broadcast_count\"],\n",
    ")\n",
    "del df_agg\n",
    "gc.collect()\n",
    "\n",
    "months_active = {n: len(s) for n, s in month_sets.items()}\n",
    "del month_sets\n",
    "gc.collect()\n",
    "\n",
    "for node in G.nodes():\n",
    "    G.nodes[node][\"sent\"] = sent_ctr.get(node, 0)\n",
    "    G.nodes[node][\"received\"] = recv_ctr.get(node, 0)\n",
    "    G.nodes[node][\"months_active\"] = months_active.get(node, 0)\n",
    "    G.nodes[node][\"unique_contacts\"] = G.degree(node)\n",
    "\n",
    "print(f\"   {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84456517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Centrality metrics â€¦\n",
      "   LCC: 19031 nodes, 173861 edges\n",
      "   Betweenness (k=200 approx) â€¦\n",
      "   PageRank â€¦\n",
      "   Eigenvector â€¦\n",
      "   Clustering (unweighted, fast) â€¦\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# CENTRALITY â€” on LCC, fast settings\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"ðŸ“ Centrality metrics â€¦\")\n",
    "lcc_nodes = max(nx.connected_components(G), key=len)\n",
    "Gcc = G.subgraph(lcc_nodes).copy()\n",
    "print(f\"   LCC: {Gcc.number_of_nodes()} nodes, {Gcc.number_of_edges()} edges\")\n",
    "\n",
    "degree_cent = nx.degree_centrality(G)\n",
    "weighted_deg = dict(G.degree(weight=\"weight\"))\n",
    "\n",
    "K = min(200, Gcc.number_of_nodes())\n",
    "print(f\"   Betweenness (k={K} approx) â€¦\")\n",
    "betweenness = nx.betweenness_centrality(\n",
    "    Gcc, weight=\"weight\", normalized=True, k=K, seed=42\n",
    ")\n",
    "\n",
    "print(\"   PageRank â€¦\")\n",
    "pagerank = nx.pagerank(Gcc, weight=\"weight\", alpha=0.85, max_iter=100, tol=1e-4)\n",
    "\n",
    "print(\"   Eigenvector â€¦\")\n",
    "try:\n",
    "    eigenvector = nx.eigenvector_centrality_numpy(Gcc, weight=\"weight\")\n",
    "except Exception:\n",
    "    eigenvector = {}\n",
    "\n",
    "print(\"   Clustering (unweighted, fast) â€¦\")\n",
    "clustering = nx.clustering(G)  # unweighted is 5-10x faster\n",
    "\n",
    "# closeness = skipped at this scale (O(VÂ·E) is too slow)\n",
    "for node in G.nodes():\n",
    "    G.nodes[node][\"degree_centrality\"] = round(degree_cent.get(node, 0), 6)\n",
    "    G.nodes[node][\"weighted_degree\"] = round(float(weighted_deg.get(node, 0)), 3)\n",
    "    G.nodes[node][\"betweenness\"] = round(betweenness.get(node, 0), 6)\n",
    "    G.nodes[node][\"pagerank\"] = round(pagerank.get(node, 0), 6)\n",
    "    G.nodes[node][\"eigenvector\"] = round(eigenvector.get(node, 0), 6)\n",
    "    G.nodes[node][\"closeness\"] = 0.0\n",
    "    G.nodes[node][\"clustering\"] = round(clustering.get(node, 0), 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49b9c6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ˜ï¸  Louvain communities â€¦\n",
      "   31 communities\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# COMMUNITY DETECTION\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"ðŸ˜ï¸  Louvain communities â€¦\")\n",
    "partition = community_louvain.best_partition(Gcc, weight=\"weight\", random_state=42)\n",
    "for node in G.nodes():\n",
    "    G.nodes[node][\"community\"] = partition.get(node, -1)\n",
    "n_communities = len(set(partition.values()))\n",
    "print(f\"   {n_communities} communities\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31e78e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ­ Role inference â€¦\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ROLE INFERENCE â€” percentile-relative thresholds\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"ðŸŽ­ Role inference â€¦\")\n",
    "pr_arr = np.array([G.nodes[n][\"pagerank\"] for n in G.nodes()])\n",
    "bet_arr = np.array([G.nodes[n][\"betweenness\"] for n in G.nodes()])\n",
    "pr_hi = float(np.quantile(pr_arr[pr_arr > 0], 0.90)) if pr_arr.any() else 0.01\n",
    "bet_hi = float(np.quantile(bet_arr[bet_arr > 0], 0.75)) if bet_arr.any() else 0.01\n",
    "\n",
    "for node in G.nodes():\n",
    "    d = G.nodes[node]\n",
    "    pr = d[\"pagerank\"]\n",
    "    bet = d[\"betweenness\"]\n",
    "    sent = d.get(\"sent\", 0)\n",
    "    recv = d.get(\"received\", 0)\n",
    "    uc = d.get(\"unique_contacts\", 0)\n",
    "    if pr >= pr_hi and bet >= bet_hi:\n",
    "        role = \"Executive / Broker\"\n",
    "    elif bet >= bet_hi:\n",
    "        role = \"Information Broker\"\n",
    "    elif sent > 500 and recv < sent * 0.3:\n",
    "        role = \"Broadcaster\"\n",
    "    elif recv > sent * 3 and recv > 200:\n",
    "        role = \"Information Sink\"\n",
    "    elif uc > 50:\n",
    "        role = \"Connector\"\n",
    "    else:\n",
    "        role = \"Regular Employee\"\n",
    "    G.nodes[node][\"inferred_role\"] = role\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38f2423b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“… Temporal / keywords â€¦\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# TEMPORAL, KEYWORDS, DEGREE DIST\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"ðŸ“… Temporal / keywords â€¦\")\n",
    "ym_ctr = Counter(ym for ym in ym_list if ym)\n",
    "timeline_data = sorted(\n",
    "    [{\"period\": k, \"email_count\": v} for k, v in ym_ctr.items()],\n",
    "    key=lambda x: x[\"period\"],\n",
    ")\n",
    "del ym_list\n",
    "gc.collect()\n",
    "\n",
    "STOP = {\n",
    "    \"re\",\n",
    "    \"fw\",\n",
    "    \"fwd\",\n",
    "    \"the\",\n",
    "    \"a\",\n",
    "    \"an\",\n",
    "    \"is\",\n",
    "    \"in\",\n",
    "    \"of\",\n",
    "    \"to\",\n",
    "    \"and\",\n",
    "    \"for\",\n",
    "    \"on\",\n",
    "    \"at\",\n",
    "    \"be\",\n",
    "    \"with\",\n",
    "    \"from\",\n",
    "    \"your\",\n",
    "    \"this\",\n",
    "    \"that\",\n",
    "    \"have\",\n",
    "    \"will\",\n",
    "    \"not\",\n",
    "    \"are\",\n",
    "    \"has\",\n",
    "    \"was\",\n",
    "    \"its\",\n",
    "    \"it\",\n",
    "    \"or\",\n",
    "    \"but\",\n",
    "    \"if\",\n",
    "    \"as\",\n",
    "    \"we\",\n",
    "    \"you\",\n",
    "}\n",
    "wc = Counter()\n",
    "for s in subj_list:\n",
    "    for w in re.findall(r\"\\b[a-z]{3,}\\b\", s.lower()):\n",
    "        if w not in STOP:\n",
    "            wc[w] += 1\n",
    "top_keywords = [{\"word\": w, \"count\": c} for w, c in wc.most_common(80)]\n",
    "del subj_list\n",
    "gc.collect()\n",
    "\n",
    "degree_dist = Counter(d for _, d in G.degree())\n",
    "degree_dist_data = [{\"degree\": k, \"count\": v} for k, v in sorted(degree_dist.items())]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "932e66e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Exporting JSON â€¦\n",
      "\n",
      "âœ… Done! Output written to: sna_output\n",
      "   total_emails: 359219\n",
      "   total_nodes: 19094\n",
      "   total_edges: 173899\n",
      "   n_communities: 31\n",
      "   density: 0.000954\n",
      "   avg_clustering: 0.2317\n",
      "   lcc_nodes: 19031\n",
      "   lcc_edges: 173861\n",
      "   date_range_start: 1979-12-31\n",
      "   date_range_end: 2002-07-12\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# EXPORT\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"ðŸ’¾ Exporting JSON â€¦\")\n",
    "top_node_ids = sorted(\n",
    "    G.nodes(), key=lambda n: G.nodes[n].get(\"weighted_degree\", 0), reverse=True\n",
    ")[:500]\n",
    "top_node_set = set(top_node_ids)\n",
    "\n",
    "nodes_out = [\n",
    "    {\n",
    "        k: G.nodes[n].get(k, 0 if k not in (\"id\", \"label\", \"inferred_role\") else \"\")\n",
    "        for k in (\n",
    "            \"id\",\n",
    "            \"label\",\n",
    "            \"sent\",\n",
    "            \"received\",\n",
    "            \"unique_contacts\",\n",
    "            \"months_active\",\n",
    "            \"degree_centrality\",\n",
    "            \"weighted_degree\",\n",
    "            \"betweenness\",\n",
    "            \"closeness\",\n",
    "            \"eigenvector\",\n",
    "            \"pagerank\",\n",
    "            \"clustering\",\n",
    "            \"community\",\n",
    "            \"inferred_role\",\n",
    "        )\n",
    "    }\n",
    "    | {\"id\": n, \"label\": n.split(\"@\")[0]}\n",
    "    for n in top_node_ids\n",
    "]\n",
    "\n",
    "edges_out = [\n",
    "    {\n",
    "        \"source\": u,\n",
    "        \"target\": v,\n",
    "        \"weight\": round(float(d.get(\"weight\", 0)), 3),\n",
    "        \"raw_count\": int(d.get(\"raw_count\", 0)),\n",
    "        \"direct_count\": int(d.get(\"direct_count\", 0)),\n",
    "        \"broadcast_count\": int(d.get(\"broadcast_count\", 0)),\n",
    "    }\n",
    "    for u, v, d in G.edges(data=True)\n",
    "    if u in top_node_set and v in top_node_set\n",
    "]\n",
    "\n",
    "top_nodes_data = [\n",
    "    {\n",
    "        \"id\": n,\n",
    "        \"sent\": d.get(\"sent\", 0),\n",
    "        \"received\": d.get(\"received\", 0),\n",
    "        \"unique_contacts\": d.get(\"unique_contacts\", 0),\n",
    "        \"pagerank\": round(d.get(\"pagerank\", 0), 6),\n",
    "        \"betweenness\": round(d.get(\"betweenness\", 0), 6),\n",
    "        \"community\": d.get(\"community\", -1),\n",
    "        \"role\": d.get(\"inferred_role\", \"Unknown\"),\n",
    "    }\n",
    "    for n, d in sorted(\n",
    "        G.nodes(data=True), key=lambda x: x[1].get(\"pagerank\", 0), reverse=True\n",
    "    )[:30]\n",
    "]\n",
    "\n",
    "comm_members = defaultdict(list)\n",
    "for n in top_node_ids:\n",
    "    comm_members[G.nodes[n].get(\"community\", -1)].append(n)\n",
    "community_summary = [\n",
    "    {\n",
    "        \"community_id\": cid,\n",
    "        \"size\": len(ms),\n",
    "        \"top_members\": sorted(\n",
    "            ms, key=lambda n: G.nodes[n].get(\"pagerank\", 0), reverse=True\n",
    "        )[:5],\n",
    "    }\n",
    "    for cid, ms in sorted(comm_members.items(), key=lambda x: -len(x[1]))\n",
    "]\n",
    "\n",
    "stats = {\n",
    "    \"total_emails\": total_ok,\n",
    "    \"total_nodes\": G.number_of_nodes(),\n",
    "    \"total_edges\": G.number_of_edges(),\n",
    "    \"n_communities\": n_communities,\n",
    "    \"density\": round(nx.density(G), 6),\n",
    "    \"avg_clustering\": round(nx.average_clustering(Gcc), 4),\n",
    "    \"lcc_nodes\": Gcc.number_of_nodes(),\n",
    "    \"lcc_edges\": Gcc.number_of_edges(),\n",
    "    \"date_range_start\": min((d for d in dates_list if d), default=\"\"),\n",
    "    \"date_range_end\": max((d for d in dates_list if d), default=\"\"),\n",
    "}\n",
    "\n",
    "SEP = (\",\", \":\")\n",
    "\n",
    "headers_df = pd.DataFrame(headers_meta, columns=[\"date\", \"from\", \"to_cc_count\", \"subject\"])\n",
    "headers_df.to_csv(f\"{OUTPUT_DIR}/email_headers_metadata.csv\", index=False)\n",
    "\n",
    "with open(f\"{OUTPUT_DIR}/graph_data.json\", \"w\") as f:\n",
    "    json.dump({\"nodes\": nodes_out, \"edges\": edges_out}, f, separators=SEP)\n",
    "with open(f\"{OUTPUT_DIR}/timeline.json\", \"w\") as f:\n",
    "    json.dump(timeline_data, f, separators=SEP)\n",
    "with open(f\"{OUTPUT_DIR}/keywords.json\", \"w\") as f:\n",
    "    json.dump(top_keywords, f, separators=SEP)\n",
    "with open(f\"{OUTPUT_DIR}/degree_dist.json\", \"w\") as f:\n",
    "    json.dump(degree_dist_data, f, separators=SEP)\n",
    "with open(f\"{OUTPUT_DIR}/top_nodes.json\", \"w\") as f:\n",
    "    json.dump(top_nodes_data, f, separators=SEP)\n",
    "with open(f\"{OUTPUT_DIR}/communities.json\", \"w\") as f:\n",
    "    json.dump(community_summary, f, separators=SEP)\n",
    "with open(f\"{OUTPUT_DIR}/stats.json\", \"w\") as f:\n",
    "    json.dump(stats, f, indent=2)\n",
    "\n",
    "print(\"\\nâœ… Done! Output written to:\", OUTPUT_DIR)\n",
    "for k, v in stats.items():\n",
    "    print(f\"   {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93467dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2026-02-21",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
